(1) Set up para el funcionamiento

(1.1) Necesitamos conda para instalar el video features (2+1)
(1.2) Insatalamos video features

(2) Pasos para los datos

(2.1) Recordar los datos a un subdataset de los unicos links que tienen subset_10. 
      Eso, para filtrar a solo los videos que vamos a usar.

(3) Análisis de los datos

(3.1) Para realizar el análisis de datos primero debemos obtener los resultados de "video_features".
(3.2) Ahora tenemos que estandarizarlo y hacerle una reducción ( etiquetar falta)
(3.3) Analisar 

--------
Estructura
- Notas
- video_features (es un link a otro github, cuando se hace clone de mi repo aparece como carpeta vacia
                  por lo que cada uno lo tiene que clonear independientemente 
                  https://github.com/v-iashin/video_features.git)
                  - I0luMKjIZyg de val es un video nulo. 
- recortar_datos (aquí eliminamos videos queno vayamos a usar para ocupar menos espacio en vano)
- read_process (aquí obtendremos los features)
- data_normalizer (compactaremos los datos de los csv para hacer un txt de puros paths que pueda ponerse
                  como parametro en --parece que video_paths no acepta solo una llena de videos--) 

--------
Choices justify:
- Video features porque ofrece varios modelos, en especial R(2+1)D el que usaremos por tener un enfoque más útil para nuestro proyecto. 
  así mismo no usamos semegantes como CLIP o S3D porque CLIP era muy pesado y entre S3D y R(2+1)D, preferimos R(2+1)D por su modelo 
  r2plus1d_34_32_ig65m_ft_kinetics que tiene un 79.10 de accuracy a comparación del de por default en R(2+1)D. 
- umap
- kmeans y kmeans++ 
  """
  ‘k-means++’ : selects initial cluster centroids using sampling based on an empirical probability distribution of the points’ 
  contribution to the overall inertia. This technique speeds up convergence. The algorithm implemented is “greedy k-means++”. It differs 
  from the vanilla k-means++ by making several trials at each sampling step and choosing the best centroid among them.
  """
  Problema: Kmeans podría llegar a seleccionar 3 puntos demasiado cercanos.
  Me lo pregunte porque por la data que pasa si tengo más datos en un cluster y por probabilidades me sale en ese lugar los 3. No pues.
